{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(data):\n",
    "    # Make the necessary imports\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from statistics import mean\n",
    "    \n",
    "    # Only select the data column for parsing\n",
    "    data = data['data']\n",
    "    \n",
    "    \n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    z_data = []\n",
    "    time_frame = []\n",
    "    device_data = []\n",
    "    battery_level = []\n",
    "    device_battery = {}\n",
    "    \n",
    "    for item in data:\n",
    "        decoded_item = item[2:-5]\n",
    "        item_value = decoded_item.split(',')\n",
    "        #print(item_value)\n",
    "        \n",
    "        # Take the device ID information\n",
    "        device_id = item_value[0]\n",
    "        \n",
    "        # The time information is in index 3 of the item_value list\n",
    "        time_ar = item_value[3]\n",
    "        times = datetime.datetime.strptime(time_ar, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        # Need to find out why we are adding timedelta of 6 hours. Is it because Dhaka is GMT+6?\n",
    "        # --> They sent date as UTC, so we've converted it into BST\n",
    "        time_val = times + datetime.timedelta(hours=6) # why the timedelta? --> using timedelta we can get the time we want and also we can convert the time into GMT+6 but using timedelta we directly converted it into our time format. \n",
    "        \n",
    "        # Take the battery information from index position 2 of the item_value list\n",
    "        # and convert it from 2-bit to 16-bit integer.\n",
    "        # --> axis data and battery information was sent as 16-bit integer, now we converted it into decimal\n",
    "        battery = np.int16(int(item_value[2], 16))\n",
    "        \n",
    "        # Take the x, y, z information from index position 1 of the item_value list \n",
    "        # and convert it from 2-bit to 16-bit integer\n",
    "        # x values between string index 0:4 for item_value[1]\n",
    "        x = (np.int16(int(item_value[1][0:4], 16)))/256 \n",
    "        # y values between string index 4:8 for item_value[1]\n",
    "        y = (np.int16(int(item_value[1][4:8], 16)))/256 \n",
    "        # x values between string index 8:12 for item_value[1]\n",
    "        z = (np.int16(int(item_value[1][8:], 16)))/256 \n",
    "        \n",
    "        # Append the battery, x, y, z, time and device information to the previously initialized lists\n",
    "        battery_level.append(battery)\n",
    "        x_data.append(x)\n",
    "        y_data.append(y)\n",
    "        z_data.append(z)\n",
    "        time_frame.append(time_val)\n",
    "        device_data.append(device_id)\n",
    "        \n",
    "    \n",
    "    # Return all the lists together in a dataframe\n",
    "    #return x_data, y_data, z_data, time_frame, device_data\n",
    "    # Create new dataframe df using the new list variables\n",
    "    df = pd.DataFrame(list(zip(device_data, time_frame, x_data, y_data, z_data)), \n",
    "                   columns =['device_id', 'time', 'x', 'y', 'z']) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(dataframe, duplicate_row_to_keep = \"first\", column_to_subset_by=\"time\"):\n",
    "   \n",
    "    # Resets the dataframe index, as a precaution. I don't like sudden jumps in index values\n",
    "    # converts time column to datetime object and sorts values in ascending order (chronologically)\n",
    "    # Removes the duplicates.\n",
    "    # 'duplicate_row_to_keep' may either be \"first\" or \"last\". The default is set to \"first\".\n",
    "    # 'column_to_subset_by' will be the column with the timestamp values. The default is set to \"time\".\n",
    "\n",
    "    \n",
    "    # Make the necessary imports\n",
    "    import pandas as pd\n",
    "    \n",
    "    #convert time column from str to datetime type\n",
    "    dataframe['time'] = pd.to_datetime(dataframe['time'])\n",
    "    dataframe = dataframe.sort_values(by=['time'])\n",
    "    \n",
    "    # remove duplicates\n",
    "    dataframe = dataframe.drop_duplicates(subset=column_to_subset_by, keep=duplicate_row_to_keep)\n",
    "    \n",
    "    # reset index\n",
    "    dataframe.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # return the duplicate removed dataframe\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_noise(dataframe, window_size=5):\n",
    "    # smoothes noise by running a rolling mean and drops the null columns\n",
    "    # window_size is set to 5 as default\n",
    "    \n",
    "    # make necessary imports \n",
    "    import pandas as pd\n",
    "    \n",
    "    dataframe['x'] = dataframe['x'].rolling(window=window_size).mean()\n",
    "    dataframe['y'] = dataframe['y'].rolling(window=window_size).mean()\n",
    "    dataframe['z'] = dataframe['z'].rolling(window=window_size).mean()\n",
    "    \n",
    "    # drop rows with null values for the smoothened data\n",
    "    dataframe = dataframe.dropna(axis=0)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the x_diff, y_diff, z_diff and sum_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_differential_values(dataframe):\n",
    "    # renames x, y, z columns to add _diff with their names\n",
    "    # calculates their differential values\n",
    "    # calculates the sum of the (absolute) differential values\n",
    "    \n",
    "    # make necessary imports\n",
    "    import pandas as pd\n",
    "    \n",
    "    # rename columns    \n",
    "    dataframe = dataframe.rename(columns={\"x\":\"x_diff\", \"y\": \"y_diff\", \"z\": \"z_diff\"})\n",
    "    \n",
    "    # turn time to index\n",
    "    dataframe = dataframe.set_index('time')\n",
    "    \n",
    "    # now calculate the differences between consecutive rows\n",
    "    dataframe = dataframe.diff(axis=0, periods=1)\n",
    "    \n",
    "    # now drop rows with na values\n",
    "    dataframe = dataframe.dropna()\n",
    "    \n",
    "    # now calculate the sum_diff\n",
    "    dataframe['sum_diff'] = abs(dataframe['x_diff']) + abs(dataframe['y_diff']) + abs(dataframe['z_diff'])\n",
    "    \n",
    "    # return the changed dataframe\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_function(km_df, number_of_clusters=3, init = \"random\", n_init=20):\n",
    "    # necessary imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # as a precaution, in case there is timestamp as index, we should reset it \n",
    "    # so we can delete it as a column in the next try block\n",
    "    try:\n",
    "        # drop the time column \n",
    "        km_df.reset_index(drop=True, inplace=True) \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # as a precaution, in case there is a timestamp column, we need to drop it\n",
    "    try:\n",
    "        # drop the time column \n",
    "        km_df = km_df.drop(columns = ['time']) \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # separate the data from the dataframe and convert to np array\n",
    "    X = km_df.values\n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    # scale the data\n",
    "    # try two different heuristics: the minmax scaler and the standard scaler \n",
    "    # and see which works better\n",
    "    # my hunch is the standard scaler should work better since the variables may have covariance\n",
    "    Clus_dataSet = StandardScaler().fit_transform(X)\n",
    "    Clus_dataSet\n",
    "    \n",
    "    # Initialize the number of clusters. My personal recommendation is 2.\n",
    "    clusterNum = number_of_clusters\n",
    "    \n",
    "    ## These are the parameters you can choose for initializing the Kmeans class\n",
    "    # init : {‘k-means++’, ‘random’ or an ndarray}\n",
    "    # Method for initialization, defaults to ‘k-means++’:\n",
    "    # ‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. \n",
    "    # See section Notes in k_init for more details.\n",
    "    # ‘random’: choose k observations (rows) at random from data for the initial centroids.\n",
    "    # If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n",
    "    # n_init : int, default: 10\n",
    "    # Number of time the k-means algorithm will be run with different centroid seeds. \n",
    "    # The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "    # max_iter : int, default: 300\n",
    "    \n",
    "    # initialize the kmeans model. \n",
    "    # for the time being, just tune the init, n_clusters and n_init parameters\n",
    "    # we'll find out more about the optimal number of clusters from the elbow method later\n",
    "    k_means = KMeans(init = init, n_clusters = clusterNum, n_init = n_init)\n",
    "    \n",
    "    # fit the model with the data\n",
    "    k_means.fit(X)\n",
    "    \n",
    "    # separate the labels\n",
    "    labels = k_means.labels_\n",
    "    \n",
    "    # add the labels into a new column to the original dataframe\n",
    "    # we need a labelled dataset to train the classifier model\n",
    "    km_df[\"Clus_km\"] = labels\n",
    "    \n",
    "    # return the labeled dataframe\n",
    "    return km_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_export_svc(svm_df, output_filename=\"/svm_for_cow.pkl\"):\n",
    "    # This function has no return value\n",
    "    # PLEASE LET ME KNOW IF THIS FUNCTION SHOULD RETURN TRAINED MODEL\n",
    "    # INSTEAD OF EXPORTING MODEL TO A FILEPATH\n",
    "    \n",
    "    # importing the libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.externals import joblib\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    # To divide the data into attributes and labels, execute the following code:\n",
    "    X = svm_df.drop(columns=['Clus_km'])\n",
    "    y = svm_df['Clus_km']\n",
    "    \n",
    "    # divide data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)\n",
    "    \n",
    "    # initialize the classifier\n",
    "    # use gridsearch on a test dataset to find the best parameters (see the script file)\n",
    "    svclassifier = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "    \n",
    "    # fit the classifier into the data\n",
    "    model = svclassifier.fit(X_train, y_train)\n",
    "    \n",
    "    # now predict on the test set using the predict method\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # save the svm model\n",
    "    path = os.getcwd()\n",
    "    joblib.dump(model, path + output_filename)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_svc(cow, classifier_filepath=\"svm_for_cow.pkl\"):\n",
    "    # Necessary imports\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    # rename columns\n",
    "    cow.rename(columns={ \"x\": \"x_diff\" }, inplace = True)\n",
    "    cow.rename(columns={ \"y\": \"y_diff\" }, inplace = True)\n",
    "    cow.rename(columns={ \"z\": \"z_diff\" }, inplace = True)\n",
    "\n",
    "    # turn time to index\n",
    "    cow = cow.set_index('time')\n",
    "\n",
    "    # now calculate the differences between consecutive rows\n",
    "    cow = cow.diff(axis=0, periods=1)\n",
    "\n",
    "    # now calculate the sum_diff\n",
    "    cow['sum_diff'] = abs(cow['x_diff']) + abs(cow['y_diff']) + abs(cow['z_diff'])\n",
    "\n",
    "    # now drop rows with na values\n",
    "    cow = cow.dropna()\n",
    "    \n",
    "    # Load model from file\n",
    "    classifier = joblib.load(classifier_filepath)\n",
    "    \n",
    "    # now predict on the test set using the predict method\n",
    "    y_pred = classifier.predict(cow)\n",
    "    \n",
    "    # add labels to the dataframe\n",
    "    labels = y_pred\n",
    "    cow['labels'] = labels\n",
    "    \n",
    "    # returned the labeled dataframe\n",
    "    return cow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(cow):\n",
    "    # imports\n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    from datetime import time\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # initialize the weight column\n",
    "    cow['weight'] = None\n",
    "    \n",
    "    # tabulate the values in the weight column\n",
    "    # assumption is cluster 0 is low activity, cluster 1 is medium activity, cluster 2 is high activity\n",
    "    for i in range(len(cow)):\n",
    "        if (cow.loc[i, 'time']==0):\n",
    "            cow.loc[i, 'weight']= 0\n",
    "        elif (cow.loc[i, 'time']==1):\n",
    "            cow.loc[i, 'weight']= 0.1\n",
    "        else:\n",
    "            cow.loc[i, 'weight']= 0.9\n",
    "            \n",
    "    # return the weight added dataframe\n",
    "    return cow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Activity Level in 1 hour time slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activity_level(cow):\n",
    "    # imports \n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    from datetime import time\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # convert time column from str to datetime type\n",
    "    # sort in ascending order (chronologically)\n",
    "    cow['time'] = pd.to_datetime(cow['time'])\n",
    "    cow = cow.sort_values(by=['time'])\n",
    "    \n",
    "    # find the initial time value\n",
    "    starting_time = cow.loc[0, 'time']\n",
    "    \n",
    "    # initialize the lists that are to be appended in the loop\n",
    "    time = []\n",
    "    activity_level = []\n",
    "    \n",
    "    # create one hour time slices\n",
    "    end_time = starting_time\n",
    "    while (end_time <= cow.iloc[len(cow)-1, 0]):\n",
    "        # create the 1-hour time range for the data to be filtered in\n",
    "        new_time = end_time\n",
    "        end_time = new_time + timedelta(hours=1) # one hour slice\n",
    "        \n",
    "        # create date filter mask\n",
    "        # greater than the start date and smaller than the end date\n",
    "        # hold the data from the 1-hour timeslice in a placeholder dataframe\n",
    "        mask = (cow['time'] > new_time) & (cow['time'] <= end_time)\n",
    "        placeholder = cow[mask]\n",
    "        \n",
    "        # summarise the value counts for the labels in the placeholder dataframe\n",
    "        summary = pd.DataFrame(placeholder['labels'].value_counts())\n",
    "        summary['cluster'] = summary.index\n",
    "        \n",
    "        # rename columns\n",
    "        summary.rename(columns={ summary.columns[0]: \"count\" }, inplace = True)\n",
    "\n",
    "        # add weights column\n",
    "        # tabulate the values in the weight column\n",
    "        # IMPORTANT: assumption is cluster 0 is low activity, cluster 1 is medium activity, cluster 2 is high activity\n",
    "        summary['weight'] = None\n",
    "        for i in range(len(summary)):\n",
    "            if (summary.iloc[i, 1]==0):\n",
    "                summary.loc[i, 'weight']=0\n",
    "            elif (summary.iloc[i, 1]==1):\n",
    "                summary.loc[i, 'weight']=0.1\n",
    "            else:\n",
    "                summary.loc[i, 'weight']=0.9\n",
    "\n",
    "        \n",
    "        # initialise the hourly activity level as an empty list\n",
    "        hourly_activity_level = []\n",
    "\n",
    "        \n",
    "        # keep appending the product of weight and value count for each cluster label to the hourly_activity_level list\n",
    "        for i in range(len(summary)):\n",
    "            activity_level.append(summary.iloc[i, 0]*summary.iloc[i, 2])\n",
    "\n",
    "        # calculate the sum of the elements in the hourly_activity_level list\n",
    "        hourly_activity_level = sum(hourly_activity_level)\n",
    "    \n",
    "        # append the time and sum of hourly activity level to the time and activity_level lists\n",
    "        time.append(new_time)\n",
    "        activity_level.append(hourly_activity_level)\n",
    "\n",
    "        # create a dataframe using the time and activity level lists\n",
    "        activity_df = pd.DataFrame(list(zip(time, activity_level)), \n",
    "                                   columns =['time', 'activity_level'])\n",
    "        \n",
    "        # convert the time column to datetime object\n",
    "        activity_df['time'] = pd.to_datetime(activity_df['time'])\n",
    "        \n",
    "        # initialise the activity level columns for the previous 1, 24, 48, and 72 hours\n",
    "        activity_df['activity_level_1'] = None\n",
    "        activity_df['activity_level_24'] = None\n",
    "        activity_df['activity_level_48'] = None\n",
    "        activity_df['activity_level_72'] = None\n",
    "        \n",
    "        for i in range(len(activity_df)):\n",
    "\n",
    "            timevalue_1 = activity_df.loc[i, 'time'] - timedelta(hours=1)\n",
    "            timevalue_24 = activity_df.loc[i, 'time'] - timedelta(hours=24)\n",
    "            timevalue_48 = activity_df.loc[i, 'time'] - timedelta(hours=48)\n",
    "            timevalue_72 = activity_df.loc[i, 'time'] - timedelta(hours=72)\n",
    "\n",
    "            # some errors arise due to duplicate time values being present in the data\n",
    "            # trying to cheat my way through with the use of try except block\n",
    "            try:  \n",
    "                activity_df.loc[i, 'activity_level_1'] = activity_df.loc[activity_df['time']==timevalue_1]['activity_level'].values[0]\n",
    "                activity_df.loc[i, 'activity_level_24'] = activity_df.loc[activity_df['time']==timevalue_24]['activity_level'].values[0]\n",
    "                activity_df.loc[i, 'activity_level_48'] = activity_df.loc[activity_df['time']==timevalue_48]['activity_level'].values[0]\n",
    "                activity_df.loc[i, 'activity_level_72'] = activity_df.loc[activity_df['time']==timevalue_72]['activity_level'].values[0]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    \n",
    "    # return the activity level dataframe\n",
    "    return activity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Activity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activity_index(activity_df):\n",
    "    # imports\n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    from datetime import time\n",
    "    from datetime import datetime\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    # initialize the historical comparison value, trend and activity index columns\n",
    "    activity_df['historical_comparison_value'] = None\n",
    "    activity_df['trend'] = None\n",
    "    activity_df['activity_index'] = None\n",
    "    \n",
    "    # calculate the historical comparison value, trend and activity index columns\n",
    "    for i in range(len(activity_df)):\n",
    "\n",
    "        # calculate the historical comparison value\n",
    "        try:\n",
    "            placeholder = 3*activity_df.loc[i, 'activity_level']\n",
    "            placeholder = placeholder - (activity_df.loc[i, 'activity_level_24'] + activity_df.loc[i, 'activity_level_48'] + activity_df.loc[i, 'activity_level_72'])\n",
    "            historical_comparison_value = placeholder/(activity_df.loc[i, 'activity_level_24'] + activity_df.loc[i, 'activity_level_48'] + activity_df.loc[i, 'activity_level_72'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # calculate the trend\n",
    "        try:\n",
    "            trend = (activity_df.loc[i, 'activity_level'] - activity_df.loc[i, 'activity_level_1'])/activity_df.loc[i, 'activity_level_1']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # insert the values for historical comparison value and trend into the dataframe\n",
    "        try:\n",
    "            activity_df.loc[i, 'historical_comparison_value'] = historical_comparison_value\n",
    "            activity_df.loc[i, 'trend'] = trend\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # calculate the activity index now\n",
    "        try:\n",
    "            activity_df.loc[i, 'activity_index'] = historical_comparison_value + trend\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # create an activity index column\n",
    "    # drop unnecessary columns\n",
    "    activity_index_df = activity_df.drop(columns=['activity_level', 'activity_level_1',\n",
    "                                             'activity_level_24', 'activity_level_48',\n",
    "                                             'activity_level_72', 'historical_comparison_value',\n",
    "                                             'trend'])\n",
    "    \n",
    "    return activity_index_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
